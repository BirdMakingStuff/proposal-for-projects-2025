\subsection{Chat Frontend using Native Toolkits for Ollama}

This aims to provide a native chatbot frontend to Ollama and other popular LLM providers such as OpenAI and Anthropic.  There are many existing projects in the market (e.g. Open WebUI) that provide a user interface to LLM servers.  However, these projects tend to require a lot of dependencies such as Docker which may not be suitable for running on low-resource terminals like the Raspberry Pi and older computers.  The plan is to provide a free/libre and open-source alternative to Open WebUI that runs natively on key Linux distros, regardless of desktop environment or window manager.  The frontend must run on X11 and ideally on Wayland as well.

A team of four to six people is desired.

\subsubsection*{Skills Desired}

The tech stack is still up for debate, particularly between Qt, PyQt, PySide, Gtk, Kivy, wxWidgets, wxPython, JavaFX, Java Swing and Neutralinojs.  The following skills are desired, but not required:

\begin{multicols}{2}
\begin{itemize}
    \item Python (PyQt, PySide, Kivy, wxPython)
    \item or JavaScript/TypeScript (Neutralinojs)
    \item or Java (JavaFX, JavaSwing)
    \item or C (Gtk)
    \item or C++ (Qt, wxWidgets)
    \item HTTP REST API
    \item SQLite
    \item Model-View-Controller
    \item Object-Oriented Programming
    \item Asynchronous Programming
    \item Git and GitHub
\end{itemize}
\end{multicols}

\subsubsection*{Time Commitment}

Each person is expected to contribute about 50 to 70 hours in total, or roughly 6 hours per week for a duration of 10 weeks.

There will be a weekly standup that is expected to take between 30 minutes and 1 hour per session.

\subsubsection*{Key Benefits}

\begin{itemize}
    \item Improved skills and knowledge in software architecture, event-driven programming and software testing
    \item Improved fundamental understanding of LLMs (e.g. parameter sizes, quantisation levels, etc)
    \item Improved communication skills
    \item A free/libre and open-source lightweight frontend for interfacing with an Ollama server
\end{itemize}
